---
layout: default
title: "" 
---


# iart AI session and H3K ML workshop 

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](http://opensource.org/licenses/MIT)
[![Twitter](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/iartag)

Main repository for the internal AI session @iart and the public H3K ML workshop.


## Schedule

* 11am - Start :smiley_cat:
* 11am - Introduction 
* 12pm - Lunch
* 12.45pm - Software setup
* 1.15pm - Experiments
* 3.15 - Presentation
* 4pm - End :crying_cat_face:


## Slides

1. [Slides for the ML workshop](https://iartag.github.io/hek-ml-workshop/slides/presentation02.html)
2. [~~Slides for the internal presentation at iart~~](https://iartag.github.io/hek-ml-workshop/slides/presentation01.html)


## Samples

The sample folder contains different examples:

* _00_styletransfer_: simple style transfer example with live webcam feed
* _01_styletransfer_: style transfer with gui + realtime filter
* _02_styletransfer_: style transfer drawing 
* _03_styletransfer_: style transfer feedback loop
* _04_mobilenet_: simple mobilenet example
* _05_cocossd_: cocossd example (box + label drawing)
* _06_maskrcnn_: simple maskrcnn example
* _07_posenet_im2txt_: The text from im2text is _"following"_ one body part
* _08_posenet_im2txt_: The text from im2text scaled / rotated according to the user hands
* _09_posenet_im2txt_: The text from im2text are turned into particles for interactions (WIP)
* _10_im2txt_attngan_: The image is described by im2txt and an image is generated by attngan
* _11_pix2pix_: pix2pix drawing
* _12_pix2pix_facelandmarks_: pix2pix face to facade (WIP)
* _~~13_cocossd_facerecognition_: (WIP)~~


## Tools

#### System requirement
Modern machine with decent hardware and sufficient space on the hard drive (20+ Gb)

#### Runway
We are using [__Runway__](https://runwayapp.ai), a tool which makes deploying ML models easy, as middleware to build the interactive experiments. All participants to the workshop should have received an invitations with some GPU credits :tada:. For those who have not installed it prior to the workshop, we will go through the [installation process](https://docs.runwayml.com/#/getting-started/installation) together.


#### Docker
[__Docker__](https://www.docker.com/) is needed in order to deploy some of the models locally. This will give us some flexibility when running experiments locally. It will also allow us to _chain_ models (at the moment a user can only run one model instance using the provided cloud GPU in Runway). A guide to getting started is [available](https://docs.runwayml.com/#/getting-started/installation?id=download-docker). For linux users, those [post install steps](https://docs.docker.com/install/linux/linux-postinstall/) could be useful as well.

> Docker for Windows requires Microsoft Hyper-V, which is supported only in the Pro, Enterprise or Education editions of Windows. If you don't have a Pro, Enterprise or Education Windows edition you will not be able to install Docker and you will be able to only run some models using cloud GPU.


#### P5.js
We will use [__p5.js__ ](https://p5js.org/) for the front end. It’s a high level creative programming framework with an [intuitive API](https://p5js.org/reference/). If some of you have used Processing before you should be confortable using p5.js. To get familiar with p5 you can go through this list of tutorials / guides:

- [P5 Learn](https://p5js.org/learn/)
- [P5 Wiki](https://github.com/processing/p5.js/wiki/)
- [Creative Coding](https://creative-coding.decontextualize.com/)
- [Shiffman's Foundation of programming in js](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6Zy51Q-x9tMWIv9cueOFTFA)
- [P5js reference](https://p5js.org/reference/)


#### Code editor
If you don’t have a code editor, please install one. Some suggestions (in no particular order)
- [Sublime Text](https://www.sublimetext.com)
- [Visual Studio](https://code.visualstudio.com)
- [Atom](https://atom.io) 


#### Web server
We need a simple web server to run the experiments locally. Some suggestions 
- If you have node.js/npm installed you can use _live-server_: `npm install -g live-server`
- [Other recommended options](https://github.com/processing/p5.js/wiki/Local-server)


## References / Reading list

* History:
  + [History - Longer history of Machine Learning](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)
  + [History - History of Machine Learning](https://cloud.withgoogle.com/build/data-analytics/explore-history-machine-learning/)
* Intro:
  + [Neural Networks - Intro videos](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
  + [Neural Networks - Intro text](https://ml4a.github.io/ml4a/neural_networks/)
  + [Machine Learning - Getting started](https://www.youtube.com/watch?v=I74ymkoNTnw)
  + [Machine Learning is fun (series)](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471)
* Books:
  + [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)
  + [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning)
  + [Intelligence Artificielles, Miroirs de nos vies (BD) ](http://www.sceneario.com/bande-dessinee/intelligences-artificielles/miroirs-de-nos-vies/29059.html)
* Tools:
  + [ML5js - Friendly Machine Learning for the Web](https://ml5js.org/)
  + [Tensorflow.js](https://www.tensorflow.org/js/)


## Repository structure

```
├── docs
│   ├── _layouts
│   ├── assets            (img, etc.. for content)
│   │   ├── css
│   │   └── images
│   └── slides            (slides of the presentations)
│       ├── demos
│       └── static        (img, etc.. for slides)
├── samples               (code sample) 
└── utilities             (scripts and notes)
